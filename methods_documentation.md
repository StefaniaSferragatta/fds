## Tree classifier
## Random Forest
## Boosting
## Stacking
## Bagging  (Bootstrap Aggregation)

### 1. Why?
Bagging offers the advantage of combining **weak learners to to outdo a single strong learner**. It also helps in the **reduction of variance**, hence **eliminating the over-fitting** of models in the procedure. If the base models trained on different samples have high variance (over-fitting), then the aggregated result would even it out thereby reducing the variance. Therefore, this technique is chosen when the base models have high variance and low bias which is generally the case with models having high degrees of freedom for complex data.

### 2. What?
Bootstrap Aggregation (Bagging), is a simple and very powerful **ensemble method**. An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model. In ensemble algorithms, bagging methods form a class of algorithms which **build several instances of a black-box estimator on random subsets** of the original training set and **then aggregate their individual predictions to form a final prediction**. Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set, when **samples are drawn with replacement**.

### 3. When?
As they provide a way to reduce over-fitting, bagging methods **work best with strong and complex models (e.g., fully developed decision trees),** in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees). Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different. When bagging with decision trees, we are less concerned about individual trees over-fitting the training data. Bagging can be used for classification and regression problems too.
 
###  4. Pro and cons?
The big **advantage** of bagging is that it can be **parallelised**. As the different models are fitted independently from each other, intensive parallelisation techniques can be used if required. 
One **disadvantage** of bagging is that it introduces a **loss of interpretability** of a model. The resultant model can experience lots of bias when the proper procedure is ignored. Despite bagging being highly accurate, it can be **computationally expensive** and this may discourage its use in certain instances.

### 5. How?
In bagging trees, the training set $L_B$ is generated by sampling from the distribution $P_L$ . Using $L_B$ a large tree T is constructed. The standard CART methodology finds the sequence of minimum cost pruned subtrees of T . The “best” pruned subtree in this sequence is selected using either cross-validation or a test set. \
- **How Many Bootstrap Replicates?** The unbagged rate is 29.1, so we are getting most of the improvement using only 10 bootstrap replicates. More than 25 bootstrap replicates is love’s labor lost.
- **How big should learning set be?**  Same size as the initial learning set L.
- **How well on knn?** The stability of nearest neighbor classification methods with respect to perturbations of
the data distinguishes them from competitors such as trees and neural nets.
- **How well on linear regression?** performance is poor if there are many small but non-zero for x coefficient.

### 6. Sklearn module
In scikit-learn, bagging methods are offered as a unified [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier "sklearn.ensemble.BaggingClassifier") meta-estimator,  taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. 
- `base_estimator` The base estimator to fit on random subsets of the dataset.
- `n_estimator`The number of base estimators in the ensemble.
- `max_samples` and `max_features` control the size of the subsets (in terms of samples and features).
- `bootstrap` and `bootstrap_features` control whether samples and features are drawn with or without replacement.
- `oob_score=True` estimate the generalization error.
- `warm_start=True` reuse the solution of the previous call to fit and add more estimators to the ensemble
- `n_jobs=int` the number of jobs to run in parallel

### 7. Useful links
original article https://link.springer.com/article/10.1023/A:1018054314350 \
youtube quick guide https://www.youtube.com/watch?v=2Mg8QD0F1dQ
